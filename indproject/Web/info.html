<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>NeiroNet</title>
  <link rel="stylesheet" href="info.css">
  <script type='text/javascript' src="/eel.js"></script>
</head>
<body>
  <div class="wrapper">
    <header>
      <nav>
        <ul class="head">
          <li><a href="main.html">Основная</a></li>     
          <li><a href="forDevops.html">Модель</a></li>
          <li><a href="info.html">Теория</a></li>
        </ul>
    </header>
    <div class="pages">
      <h1 id="gey">Оглавление</h1>
      <ol>
        <li><a href="#perceptron">Перцептрон</a></li>
        <li><a href="#back_propagation">Алгоритм Back propagation</a></li>
        <li><a name="perceptron"></a><a href="#func_actiovation">Функции активации</a></li>
        <li><a href="#CNN">Свёрточные нейронные сети</a></li>
      </ol>
    </div>
    <div class="perceptron">
        <h1>Перцептрон</h1>
        Простейшая НС – персептрон, представляет собой упрощенное 
        отражение работы биологической сети, состоящей из нейронов, 
        соединенных между собой дендридами и аксонами:<br>
        <img src="Image/perceptron.png" alt="Произошла неизвестная ошибка, 
        просим сообщить об этом разработчику"><br>
        <div class="sign">Простейшая модель Перцептрона</div><br>
        Это классический пример полносвязной сети прямого распространения. 
        Здесь каждый нейрон предыдущего слоя связан с каждым нейроном следующего слоя. 
        А сигнал распространяется от входного слоя к выходному, 
        не образуя обратных связей.<br>
        <p>Каждая связь между нейронами имеет определенный вес и, сигнал, проходя по ней, 
        меняет свое значение в соответствии с этим весом:</p><br>
        <img src="Image/neiron.png" alt="Произошла неизвестная ошибка, 
        просим сообщить об этом разработчику"><br>
        <div class="sign">Устройство нейрона</div><br>
        Сам по себе нейрон – это сумматор входных сигналов, который, затем, пропускает
        сумму через функцию , называемую функцией активации. 
        Выходное значение этой функции и есть выходное значение нейрона.
        <img src="Image/activation.png" width="240" height="96" alt="Произошла неизвестная ошибка, 
        просим сообщить об этом разработчику">
        <div class="sign"><a name="back_propagation"></a>Пример функции активации</div><br>

        <h1>Алгоритм Back propagation</h1>
        В относительно маленьких нейронный сетях можно подобрать веса связей вручную 
        Но при увеличении числа нейронов и связей, ручной подбор становится попросту 
        невозможным и возникает задача нахождения весовых коэффициентов связей нейросетей. 
        Этот процесс называют обучением нейронной сети.<br>
        <p>Самым популярным подходом к обучением нейросетей является алгоритм обратного 
        распространения ошибки (англ. Back propagation), который в свою очередь базируется на методе градиентного спуска.<br></p> 
        Один из распространенных подходов к обучению заключается в последовательном предъявлении НС
        векторов наблюдений и последующей корректировки весовых коэффициентов так, 
        чтобы выходное значение совпадало с требуемым:
        Это называется обучение с учителем, так как для каждого вектора 
        мы знаем нужный ответ и именно его требуем от нашей НС.        
        <p>Градиент - это вектор, указывающий направление и скорость изменения функции.<br>
        Градиент вычсляется по следующей функции:</p>
        <img src="Image/gradient.png" width="200" height="60" alt="Произошла неизвестная ошибка, 
        просим сообщить об этом разработчику">
        Где "e" - ошибка нейросети (ожидаемый ответ - полученный ответ),<br> 
        а "f'(v<sub>out</sub>)" - производная функции в точке полученного вывода нейросети.<br>
        <p>Посчитав локальный градиент, мы можем найти необходимое изменение весов связей,
        перемножив локальный градиент на текущее значение связи.<br>
        Прокрутив этот алгоритм несколько тысяч раз мы сможем верно обучить нейросеть.</p>
        <a name="func_actiovation"></a>Такое обучение может занять некоторое время, чем больше итераций пройдёт тем лучше обучится нейросеть.
          
        <h1>Функции активации</h1>
        На данный момент существует несколько сотен различных вариантов.
        Некоторые из них эффективные, другие уже устарели. Одни используются для задач классификации,
        другие для прогнозирования событий. На данный момент не существует общепризнанного, лучшего
        варианта, а поэтому выбор активационной функции - одна из важнейших стадий создания структуры сети.<br>
        Самые известные и эффективные функции активации:
            <div class="func">Пороговая функция</div>
              <img src="Image/step_func.png" width="267" height="321" alt="Произошла неизвестная ошибка, 
              просим сообщить об этом разработчику">
              Самая классическая и одновременно самая старая функция активации - пороговая:<br>
              Функция принимает значение 1 (активирована), когда x > 0 (граница), и значение 0 (не активирована) в противном случае.
              Всё очень просто и понятно, но такой подход имеет свои минусы: так f(x) может принимать только два разных значения,
              а если потребуется больше? Также нейрон с такой активационной функцией невозможно обучить, потому что, как было описано выше,
              значения весов изменются в зависимости от их производной, а в этой функции производная всегда равна 0.
            <div class="func">Сигмойда</div>
              <img src="Image/logic_func.png" width="267" height="321" alt="Произошла неизвестная ошибка, 
              просим сообщить об этом разработчику">
              Сигмоида выглядит гладкой и подобна ступенчатой функции. Рассмотрим её преимущества.<br>
              Во-первых, сигмоида — нелинейна по своей природе, а значит возможно её обучение, так как производная не равна 0.
              Еще одно достоинство такой функции — она не бинарна, что делает активацию аналоговой, в отличие от ступенчатой функции.
              Сигмоида действительно выглядит подходящей функцией для задач классификации.
              Сегодня сигмоида является одной из самых частых активационных функций в нейросетях. Но и у неё есть недостатки, на которые стоит обратить внимание.
              Вы уже могли заметить, что при приближении к концам сигмоиды значения Y имеют тенденцию слабо реагировать на изменения в X. Это означает, 
              что градиент в таких областях принимает маленькие значения, а это значит, что обучение будет происходить крайне медленно.
            <div class="func">Гиперболический тангенс</div>
              <img src="Image/tang_func.png" width="267" height="321" alt="Произошла неизвестная ошибка, 
              просим сообщить об этом разработчику">
              Еще одна часто используемая активационная функция — гиперболический тангенс.
              Гиперболический тангенс очень похож на сигмоиду. И действительно, это скорректированная сигмоидная функция.
              Поэтому такая функция имеет те же характеристики, что и у сигмоиды, рассмотренной ранее. 
            <div class="func">Функция ReLu</div>
              <img src="Image/relu_func.png" width="346" height="203" alt="Произошла неизвестная ошибка, 
              просим сообщить об этом разработчику">
              Это одна из самых популярных функций активации на сегодняшний день и, кроме того, не уменьшает локальные градиенты при переходе от слоя к слою. 
              Поэтому часто используется при deep learning – обучении НС с большим числом слоев.
              Также ReLu менее требовательно к вычислительным ресурсам, чем гиперболический тангенс или сигмоида, 
              так как производит более простые математические операции. <a name="CNN"></a>Поэтому имеет смысл использовать ReLu при создании глубоких нейронных сетей.
        <h1>Свёрточные нейронные сети</h1>
        Одна из основных сфер применения нейронных сетей является классификации графических образов (отличать кошку от собаки, самолет от автомобиля, 
        мужчин от женщин и т.д.), делать стилизацию изображений, выполнять их раскраску, генерировать новые графические образы и делать много других интересных вещей с изображениями. 
        Обычной перцептрон не слишком хорошо справляется с этими задачами. 
        Когда речь заходит об обработке изображений, то используется особая архитектура – сверточные нейросети.
        Общая идея архитектуры таких сетей была подсмотрена у биологической зрительной системы. Ученые выяснили, что дендриды каждого нейрона соединяются не со всеми  рецепторами 
        сетчатки глаза, а лишь с некоторой локальной областью. И уже дендриды всей группы зрительных нейронов покрывают сетчатку глаза целиком:
        <img src="Image/eye.png" alt="Произошла неизвестная ошибка, 
        просим сообщить об этом разработчику">
        Математики обобщили эту структуру и предложили следующее решение. Входной сигнал изображения подается на вход нейрона только в пределах ограниченной области, как правило, 
        квадратной, например, 3х3 пикселей. Затем, эта область смещается вправо на заданный шаг, допустим, 
        1 пиксель и входы подаются уже на второй нейрон. Так происходит сканирование всего изображения. Причем, весовые коэффициенты для всех нейронов этой группы – одинаковые.
        После этого сканирование изображения повторяется, но с другим набором весовых коэффициентов. Получаем вторую группу нейронов. 
        Затем, третью, четвертую и в общем случае имеем n различных групп. Так формируется первый скрытый слой нейронов сверточной НС.
        <img src="Image/cnn.png" alt="Произошла неизвестная ошибка, 
        просим сообщить об этом разработчику">
        Каждая «маска» ищет свои признаки, после чего картинка собирается заново и на основе найденных признаков нейросеть делает выводы.
    <div class="undersign">Для изучения более подробной информации, рекомендую прочитать книги: «Машинное обучение» - Бринк Х., Ричардс Д., Феверолф М
      или «Крупномасштабное машинное обучение вместе с Python» - Шарден Б., Массарон Л., Боскетти А.
    </div>
    </div>
    <script src="info.js"></script>
</body>
</html>